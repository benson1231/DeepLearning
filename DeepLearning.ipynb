{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqpwDUTega6KkdymtBjVIH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benson1231/DeepLearning/blob/main/DeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor: 深度學習中的資料結構\n",
        "\n",
        "在深度學習中，資料結構被稱為 **張量（Tensor）**。張量是一種多維陣列，用於表示標量、向量、矩陣以及更高維度的數據。\n",
        "\n",
        "---\n",
        "\n",
        "## 1. 張量的基本結構\n",
        "\n",
        "### **1.1 標量（Scalars）**\n",
        "- 單一的數值，只有一個數據點。\n",
        "- **維度（Rank）**: 0\n",
        "- **例子**: \\( 3, -1, \\pi \\)\n",
        "\n",
        "---\n",
        "\n",
        "### **1.2 向量（Vectors）**\n",
        "- 一維的數據結構，可以包含多個數值。\n",
        "- **維度（Rank）**: 1\n",
        "- **例子**: \\( [1, 2, 3] \\), \\( [-5, 0, 8] \\)\n",
        "\n",
        "---\n",
        "\n",
        "### **1.3 矩陣（Matrices）**\n",
        "- 二維的數據結構，由行（rows）和列（columns）組成。\n",
        "- **維度（Rank）**: 2\n",
        "- **例子**:\n",
        "  \\[\n",
        "  \\begin{bmatrix}\n",
        "  1 & 2 & 3 \\\\\n",
        "  4 & 5 & 6\n",
        "  \\end{bmatrix}\n",
        "  \\]\n",
        "\n",
        "---\n",
        "\n",
        "### **1.4 高維張量（Higher-order Tensors）**\n",
        "- 超過二維的多維陣列。\n",
        "- **維度（Rank）**: 3 或更高\n",
        "- **例子**: 用於表示圖片資料的四維張量，形狀為 \\( [批次大小, 高度, 寬度, 通道數] \\)。\n",
        "\n",
        "---\n",
        "\n",
        "## 2. 深度學習中的張量應用\n",
        "\n",
        "- **輸入數據**: 用張量表示圖片、文字、音頻等資料。\n",
        "- **權重與偏差**: 張量用於存儲神經網絡的參數。\n",
        "- **輸出數據**: 模型的預測結果也以張量形式表現。\n",
        "\n",
        "---\n",
        "\n",
        "## 3. 張量與代數運算\n",
        "\n",
        "張量支持多種運算，包括但不限於：\n",
        "- **加法**: \\( A + B \\)\n",
        "- **內積**: \\( A \\cdot B \\)\n",
        "- **轉置**: \\( A^T \\)\n",
        "- **廣播（Broadcasting）**: 自動對齊不同形狀的張量。\n",
        "\n",
        "---\n",
        "\n",
        "## 4. 張量的表示\n",
        "\n",
        "在 Python 中，我們通常使用 **NumPy** 或 **TensorFlow/PyTorch** 來操作張量。例如：\n",
        "\n"
      ],
      "metadata": {
        "id": "6cniaR5CuBwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 創建一個二維張量\n",
        "matrix = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
        "print(matrix.shape)  # 輸出: (2, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcM6vARbyYVO",
        "outputId": "a760196a-b302-401b-87d2-3113a829f720"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 常見的隱藏層激活函數\n",
        "\n",
        "在神經網絡中，激活函數用於引入非線性，使模型能學習複雜的數據模式。\n",
        "\n",
        "## 1. ReLU（Rectified Linear Unit）\n",
        "- **公式**: \\( f(x) = \\max(0, x) \\)\n",
        "- **範圍**: \\( [0, \\infty) \\)\n",
        "- **優點**:\n",
        "  - 計算效率高。\n",
        "  - 避免梯度消失問題。\n",
        "- **缺點**:\n",
        "  - 可能導致神經元死亡。\n",
        "- **用途**: 最常用的隱藏層激活函數。\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Leaky ReLU\n",
        "- **公式**:\n",
        "  \\[\n",
        "  f(x) =\n",
        "  \\begin{cases}\n",
        "  x, & x > 0 \\\\\n",
        "  \\alpha x, & x \\leq 0\n",
        "  \\end{cases}\n",
        "  \\]\n",
        "  (\\( \\alpha > 0 \\))\n",
        "- **範圍**: \\( (-\\infty, \\infty) \\)\n",
        "- **優點**: 解決 ReLU 的神經元死亡問題。\n",
        "- **用途**: 深層模型或數據分布不均時。\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Sigmoid\n",
        "- **公式**: \\( f(x) = \\frac{1}{1 + e^{-x}} \\)\n",
        "- **範圍**: \\( (0, 1) \\)\n",
        "- **優點**:\n",
        "  - 將輸出壓縮到 \\( (0, 1) \\)，類似概率。\n",
        "- **缺點**:\n",
        "  - 容易導致梯度消失。\n",
        "- **用途**: 很少用於隱藏層，多用於輸出層。\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Tanh（Hyperbolic Tangent）\n",
        "- **公式**: \\( f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\)\n",
        "- **範圍**: \\( (-1, 1) \\)\n",
        "- **優點**:\n",
        "  - 比 Sigmoid 更適合隱藏層。\n",
        "- **缺點**:\n",
        "  - 容易遇到梯度消失問題。\n",
        "- **用途**: 特殊情況下用於隱藏層。\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Softplus\n",
        "- **公式**: \\( f(x) = \\ln(1 + e^x) \\)\n",
        "- **範圍**: \\( (0, \\infty) \\)\n",
        "- **優點**: 是 ReLU 的平滑版本。\n",
        "- **缺點**: 計算效率較低。\n",
        "- **用途**: 需要平滑激活的場景。\n",
        "\n",
        "---\n",
        "\n",
        "## 6. ELU（Exponential Linear Unit）\n",
        "- **公式**:\n",
        "  \\[\n",
        "  f(x) =\n",
        "  \\begin{cases}\n",
        "  x, & x > 0 \\\\\n",
        "  \\alpha(e^x - 1), & x \\leq 0\n",
        "  \\end{cases}\n",
        "  \\]\n",
        "- **範圍**: \\( (-\\alpha, \\infty) \\)\n",
        "- **優點**:\n",
        "  - 解決 ReLU 對負值的問題。\n",
        "- **用途**: 需要更高穩定性時。\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Swish\n",
        "- **公式**: \\( f(x) = x \\cdot \\text{sigmoid}(x) \\)\n",
        "- **範圍**: \\( (-\\infty, \\infty) \\)\n",
        "- **優點**: 性能優異，曲線平滑。\n",
        "- **用途**: 高性能深度學習應用。\n",
        "\n",
        "---\n",
        "\n",
        "## 總結\n",
        "| 激活函數 | 範圍 | 優點 | 缺點 | 使用場景 |\n",
        "| -------- | ---- | ---- | ---- | -------- |\n",
        "| ReLU | \\( [0, \\infty) \\) | 計算高效，梯度非零 | 神經元死亡問題 | 最通用 |\n",
        "| Leaky ReLU | \\( (-\\infty, \\infty) \\) | 避免死亡問題 | 較少用於特殊場合 | 替代 ReLU |\n",
        "| Sigmoid | \\( (0, 1) \\) | 類似概率 | 梯度消失 | 輸出層 |\n",
        "| Tanh | \\( (-1, 1) \\) | 較適合隱藏層 | 梯度消失 | 特殊隱藏層 |\n",
        "| Softplus | \\( (0, \\infty) \\) | 平滑 ReLU | 計算較慢 | 平滑需求 |\n",
        "| ELU | \\( (-\\alpha, \\infty) \\) | 負值穩定 | 速度稍慢 | 穩定需求 |\n",
        "| Swish | \\( (-\\infty, \\infty) \\) | 平滑，性能強 | 較新，理解難 | 高性能應用 |\n"
      ],
      "metadata": {
        "id": "YqoaHgyPuB0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 損失函數介紹\n",
        "\n",
        "損失函數是機器學習模型中用於衡量預測結果與實際目標之間差距的指標。以下介紹兩種常見的損失函數及其應用場景。\n",
        "\n",
        "---\n",
        "\n",
        "## 1. 均方誤差（Mean Squared Error, MSE）\n",
        "\n",
        "### **應用場景**\n",
        "均方誤差通常用於 **線性迴歸** 或其他回歸問題中。它通過計算預測值與實際值之間的平方差來衡量模型的性能。\n",
        "\n",
        "### **公式**\n",
        "\\[\n",
        "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "\\]\n",
        "- \\( y_i \\): 實際值\n",
        "- \\( \\hat{y}_i \\): 預測值\n",
        "- \\( n \\): 樣本數\n",
        "\n",
        "### **計算過程**\n",
        "1. 計算每個數據點的預測值與實際值之間的差異。\n",
        "2. 將每個差異平方。\n",
        "3. 計算所有平方差的平均值。\n",
        "\n",
        "### **視覺化**\n",
        "假設有一條 **最佳擬合線**，均方誤差會顯示每個數據點到該線的 **平方距離**。這些平方距離越小，模型擬合效果越好。\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 2. 交叉熵損失（Cross-Entropy Loss）\n",
        "\n",
        "### **應用場景**\n",
        "交叉熵損失主要用於 **分類問題**，尤其是多類分類模型，例如 **神經網絡中的分類學習模型**。\n",
        "\n",
        "### **公式**\n",
        "對於二元分類：\n",
        "\\[\n",
        "\\text{Cross-Entropy Loss} = - \\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
        "\\]\n",
        "- \\( y_i \\): 實際類別（0 或 1）\n",
        "- \\( \\hat{y}_i \\): 預測類別的概率\n",
        "- \\( n \\): 樣本數\n",
        "\n",
        "對於多類分類：\n",
        "\\[\n",
        "\\text{Cross-Entropy Loss} = - \\sum_{i=1}^{n} \\sum_{j=1}^{k} y_{ij} \\log(\\hat{y}_{ij})\n",
        "\\]\n",
        "- \\( k \\): 類別數\n",
        "\n",
        "### **特點**\n",
        "- 強調預測值與實際概率分佈的匹配。\n",
        "- 應用於 **Softmax 激活函數** 的輸出層，對多類分類模型表現良好。\n",
        "\n",
        "---\n",
        "\n",
        "## **損失函數對比**\n",
        "\n",
        "| 損失函數        | 應用場景   | 優勢                  | 劣勢                   |\n",
        "| ---------------- | ---------- | --------------------- | ---------------------- |\n",
        "| 均方誤差（MSE） | 回歸問題   | 計算簡單，易於理解    | 對離群值敏感           |\n",
        "| 交叉熵損失       | 分類問題   | 適用分類，效果穩定    | 需要概率分佈的預測輸出 |\n",
        "\n",
        "---\n",
        "\n",
        "透過選擇合適的損失函數，可以更有效地指導模型訓練並提升性能。\n"
      ],
      "metadata": {
        "id": "1rbg8D3xwMnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "ex_1_true = [1, 0, 0]  # 第一類\n",
        "ex_2_true = [0, 1, 0]  # 第二類\n",
        "ex_3_true = [0, 0, 1]  # 第三類\n",
        "\n",
        "ex_1_predicted = [0.7, 0.2, 0.1]  # 準確預測\n",
        "ex_1_predicted_bad = [0.1, 0.1, 0.7]  # 錯誤預測\n",
        "\n",
        "ll = log_loss(true_labels, predicted_labels)\n",
        "print('Average Log Loss (good prediction): %.3f' % ll)\n",
        "\n",
        "ll = log_loss(true_labels, predicted_labels_bad)\n",
        "print('Average Log Loss (bad prediction): %.3f' % ll)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mA0EJsvEtki",
        "outputId": "766d5593-93b5-4020-b5a5-d775c11449f3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Log Loss (good prediction): 0.364\n",
            "Average Log Loss (bad prediction): 2.072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 優化算法介紹\n",
        "\n",
        "在機器學習中，優化算法用於調整模型參數以最小化損失函數。以下介紹三種常見的優化算法：**梯度下降法（GD）**、**隨機梯度下降法（SGD）** 以及 **Adam**。\n",
        "\n",
        "---\n",
        "\n",
        "## 1. 梯度下降法（Gradient Descent, GD）\n",
        "\n",
        "### **原理**\n",
        "梯度下降通過計算損失函數相對模型參數的梯度，沿著梯度的反方向更新參數，逐步逼近最優解。\n",
        "\n",
        "### **公式**\n",
        "\\[\n",
        "\\theta = \\theta - \\eta \\cdot \\nabla_{\\theta} J(\\theta)\n",
        "\\]\n",
        "- \\( \\theta \\): 模型參數\n",
        "- \\( \\eta \\): 學習率\n",
        "- \\( \\nabla_{\\theta} J(\\theta) \\): 損失函數對參數的梯度\n",
        "\n",
        "### **特點**\n",
        "- **優點**: 能穩定收斂到全局最優解（對於凸問題）。\n",
        "- **缺點**: 每次更新需要使用整個數據集進行計算，計算成本高，特別是對大數據集。\n",
        "\n",
        "---\n",
        "\n",
        "## 2. 隨機梯度下降法（Stochastic Gradient Descent, SGD）\n",
        "\n",
        "### **原理**\n",
        "SGD 是梯度下降的一種變體，每次更新使用單個樣本計算梯度，而不是整個數據集。\n",
        "\n",
        "### **公式**\n",
        "\\[\n",
        "\\theta = \\theta - \\eta \\cdot \\nabla_{\\theta} J(\\theta; x^{(i)}, y^{(i)})\n",
        "\\]\n",
        "- \\( x^{(i)}, y^{(i)} \\): 單一樣本及其標籤\n",
        "\n",
        "### **特點**\n",
        "- **優點**: 計算效率高，適合大數據集。\n",
        "- **缺點**: 可能出現梯度波動，導致收斂不穩定。\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Adam（Adaptive Moment Estimation）\n",
        "\n",
        "### **原理**\n",
        "Adam 結合了 **動量法** 和 **RMSProp**，通過調整每個參數的學習率，自適應地更新參數。\n",
        "\n",
        "### **公式**\n",
        "1. **一階動量（均值）**:\n",
        "   \\[\n",
        "   m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla_{\\theta} J(\\theta)\n",
        "   \\]\n",
        "2. **二階動量（方差）**:\n",
        "   \\[\n",
        "   v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla_{\\theta} J(\\theta))^2\n",
        "   \\]\n",
        "3. **參數更新**:\n",
        "   \\[\n",
        "   \\theta = \\theta - \\eta \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
        "   \\]\n",
        "   - \\( \\hat{m}_t, \\hat{v}_t \\): 偏差修正的一階和二階動量\n",
        "   - \\( \\epsilon \\): 防止分母為零的小常數\n",
        "\n",
        "### **特點**\n",
        "- **優點**: 收斂速度快，適合稀疏數據和非平滑目標函數。\n",
        "- **缺點**: 超參數需要調整，可能過早收斂到次優解。\n",
        "\n",
        "---\n",
        "\n",
        "## **算法對比**\n",
        "\n",
        "| 優化算法         | 計算效率           | 收斂穩定性      | 適用場景               |\n",
        "| ---------------- | ------------------ | --------------- | ---------------------- |\n",
        "| 梯度下降（GD）   | 慢（需整個數據集） | 穩定             | 小型數據集             |\n",
        "| 隨機梯度下降（SGD） | 快（僅需單個樣本） | 收斂不穩定       | 大型數據集，流式數據   |\n",
        "| Adam             | 快（自適應學習率） | 收斂穩定，快速   | 稀疏數據，非平滑問題   |\n",
        "\n",
        "---\n",
        "\n",
        "透過選擇適當的優化算法，可以在計算效率與收斂效果間取得平衡，從而提升模型訓練效果。\n"
      ],
      "metadata": {
        "id": "DnItxaFMxBTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 神經網路學習管道概述\n",
        "\n",
        "---\n",
        "\n",
        "## **1. 分類與迴歸任務**\n",
        "\n",
        "### **分類（Classification）**\n",
        "- **定義**: 給定資料和每個資料點的真實標籤或類別，訓練模型來預測每個資料範例的標籤。\n",
        "- **應用案例**:\n",
        "  - 例如，根據過去火災危險的數據，模型可以學習預測某一天是否可能發生火災。\n",
        "- **輸出**: 類別標籤（離散值）。\n",
        "\n",
        "---\n",
        "\n",
        "### **迴歸（Regression）**\n",
        "- **定義**: 給定資料和每個資料點的真實連續值，訓練模型來預測每個資料範例的連續值。\n",
        "- **應用案例**:\n",
        "  - 例如，根據先前的股票市場數據，預測特定時間點的股票價格。\n",
        "- **輸出**: 連續值。\n",
        "\n",
        "---\n",
        "\n",
        "## **2. 神經網路學習的主要組件**\n",
        "\n",
        "### **2.1 輸入資料**\n",
        "- **定義**: 訓練神經網路模型所需的資料。\n",
        "- **作用**: 模型從資料中學習特徵與目標值之間的關係。\n",
        "\n",
        "---\n",
        "\n",
        "### **2.2 優化器（Optimizer）**\n",
        "- **定義**: 一種演算法，用於根據訓練資料調整網路參數（如權重和偏差）。\n",
        "- **常見優化器**:\n",
        "  - **隨機梯度下降（SGD）**\n",
        "  - **Adam**\n",
        "  - **RMSprop**\n",
        "\n",
        "---\n",
        "\n",
        "### **2.3 損失函數或成本函數**\n",
        "- **定義**: 衡量模型在訓練資料上的表現，用於告訴優化器如何改進。\n",
        "- **常見損失函數**:\n",
        "  - **分類**: 交叉熵損失（Cross-Entropy Loss）\n",
        "  - **迴歸**: 均方誤差（Mean Squared Error, MSE）\n",
        "- **作用**: 損失函數的值越小，模型的預測越接近真實值。\n",
        "\n",
        "---\n",
        "\n",
        "### **2.4 評估指標**\n",
        "- **定義**: 測量模型在驗證資料上的表現。\n",
        "- **作用**: 提供模型性能的額外衡量標準，用於選擇最佳模型。\n",
        "- **常見指標**:\n",
        "  - **分類**: 準確率（Accuracy）、F1 分數\n",
        "  - **迴歸**: 平均絕對誤差（Mean Absolute Error, MAE）\n",
        "\n",
        "---\n",
        "\n",
        "## **3. 神經網路參數與學習過程**\n",
        "\n",
        "- **參數模型**: 神經網路是參數模型，包含權重和偏差等變數。\n",
        "- **訓練過程**:\n",
        "  - 使用輸入資料調整參數，使損失函數最小化。\n",
        "  - 利用優化器找到最佳參數配置。\n",
        "- **模型評估**:\n",
        "  - 在訓練期間未見過的測試資料上，評估模型效能。\n",
        "\n",
        "---\n",
        "\n",
        "透過這些組件的協同作用，神經網路可以高效學習並在分類與迴歸問題中進行準確預測。\n"
      ],
      "metadata": {
        "id": "mQTbMrLwzIp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 數據預處理步驟概述\n",
        "\n",
        "---\n",
        "\n",
        "## **1. One-hot 編碼**\n",
        "\n",
        "### **定義**\n",
        "由於神經網路無法直接處理字串數據，我們需要將分類特徵轉換為數值特徵。One-hot 編碼是常用的技術，為每個類別建立一個二進位列。\n",
        "\n",
        "### **範例**\n",
        "假設分類特徵為“區域”，共有四個類別：\n",
        "- 東北、東南、西北、西南\n",
        "\n",
        "One-hot 編碼的結果如下：\n",
        "| 東北 | 西北 | 東南 | 西南 |\n",
        "|------|------|------|------|\n",
        "|  1   |  0   |  0   |  0   |\n",
        "|  0   |  1   |  0   |  0   |\n",
        "\n",
        "### **實現**\n",
        "透過 `pandas` 的 `get_dummies()` 函數可完成 One-hot 編碼：\n",
        "```python\n",
        "features = pd.get_dummies(features)\n"
      ],
      "metadata": {
        "id": "o58qspUv0JcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 原始標籤 (0~9 的數字)\n",
        "y_train = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "# 轉換為 one-hot 編碼\n",
        "y_train_one_hot = to_categorical(y_train, num_classes=10)\n",
        "\n",
        "print(\"原始標籤:\", y_train)\n",
        "print(\"One-hot 編碼:\")\n",
        "print(y_train_one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp6OaMZL5vM4",
        "outputId": "76d1727b-0131-4509-c568-302cb3a63adc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "原始標籤: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "One-hot 編碼:\n",
            "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    }
  ]
}